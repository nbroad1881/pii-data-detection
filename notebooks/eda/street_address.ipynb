{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env file!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "dotenv_path = Path(\"../../.env\")\n",
    "if dotenv_path.exists():\n",
    "    print(\"Loaded .env file!\")\n",
    "    load_dotenv(str(dotenv_path))\n",
    "\n",
    "\n",
    "data = json.load(open(Path(os.environ[\"PROJECT_HOME_DIR\"]) / \"data/train.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591 Smith Centers Apt. 656\n",
      "Joshuamouth, RI 95963 \n",
      "**********\n",
      "743 Erika Bypass Apt. 419\n",
      "Andreahaven, IL 54207\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "for d in data:\n",
    "    temp = \"\"\n",
    "    for token, ll, ws in zip(d[\"tokens\"], d[\"labels\"], d[\"trailing_whitespace\"]):\n",
    "        if \"STREET\" in ll:\n",
    "            temp += token\n",
    "            if ws:\n",
    "                temp += \" \"\n",
    "\n",
    "    if temp != \"\":\n",
    "        print(temp)\n",
    "        print(\"*\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'591', '743'}\n",
      "**********\n",
      "{'95963', '656', '54207', '.', 'Smith', '\\n', 'Bypass', 'Andreahaven', 'IL', 'Apt', ',', '419', 'Joshuamouth', 'Erika', 'Centers', 'RI'}\n"
     ]
    }
   ],
   "source": [
    "b_tokens = set()\n",
    "i_tokens = set()\n",
    "\n",
    "for d in data:\n",
    "    for token, ll in zip(d[\"tokens\"], d[\"labels\"]):\n",
    "        if \"STREET\" in ll:\n",
    "            if ll.startswith(\"B-\"):\n",
    "                b_tokens.add(token)\n",
    "            if ll.startswith(\"I-\"):\n",
    "                i_tokens.add(token)\n",
    "\n",
    "print(b_tokens)\n",
    "print(\"*\"*10)\n",
    "print(i_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking if preprocessing labels newlines correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/miniconda3/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983b5b2ef26d444694030665fe95126d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from piidd.processing.pre import strided_tokenize\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "temp = [d for d in data if any([\"STREET\" in ll for ll in d[\"labels\"]])]\n",
    "\n",
    "ds1 = Dataset.from_dict(\n",
    "        {\n",
    "            \"full_text\": [x[\"full_text\"] for x in temp],\n",
    "            \"document\": [x[\"document\"] for x in temp],\n",
    "            \"tokens\": [x[\"tokens\"] for x in temp],\n",
    "            \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in temp],\n",
    "            \"provided_labels\": [x[\"labels\"] for x in temp],\n",
    "        }\n",
    "    )\n",
    "\n",
    "labels = set()\n",
    "for d in ds1[\"provided_labels\"]:\n",
    "    for ll in d:\n",
    "        labels.add(ll)\n",
    "\n",
    "labels = list(labels)\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "\n",
    "\n",
    "tds = ds1.map(\n",
    "    strided_tokenize,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"stride\": 128, \"max_length\": 512, \"label2id\": label2id}, \n",
    "    batch_size=1,\n",
    "    remove_columns=ds1.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁743', 5),\n",
       " ('▁Erika', 0),\n",
       " ('▁Bypass', 0),\n",
       " ('▁Apt', 0),\n",
       " ('.', 0),\n",
       " ('▁419', 0),\n",
       " ('▁Andrea', 0),\n",
       " ('haven', 0),\n",
       " (',', 0),\n",
       " ('▁IL', 0),\n",
       " ('▁54', 0),\n",
       " ('207', 0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2\n",
    "\n",
    "ids = tds[idx][\"input_ids\"]\n",
    "labels = tds[idx][\"labels\"]\n",
    "\n",
    "[(x,y) for x,y in list(zip(tokenizer.convert_ids_to_tokens(ids), [labels[i] for i in range(len(labels))])) if y in {label2id[\"I-STREET_ADDRESS\"], label2id[\"B-STREET_ADDRESS\"]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b941daa728c4a24a089639338326b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizers import AddedToken\n",
    "\n",
    "tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n",
    "\n",
    "tds2 = ds1.map(\n",
    "    strided_tokenize,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"stride\": 128, \"max_length\": 512, \"label2id\": label2id}, \n",
    "    batch_size=1,\n",
    "    remove_columns=ds1.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁743', 5),\n",
       " ('▁Erika', 0),\n",
       " ('▁Bypass', 0),\n",
       " ('▁Apt', 0),\n",
       " ('.', 0),\n",
       " ('▁419', 0),\n",
       " ('\\n', 0),\n",
       " ('▁Andrea', 0),\n",
       " ('haven', 0),\n",
       " (',', 0),\n",
       " ('▁IL', 0),\n",
       " ('▁54', 0),\n",
       " ('207', 0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2\n",
    "\n",
    "ids = tds2[idx][\"input_ids\"]\n",
    "labels = tds2[idx][\"labels\"]\n",
    "\n",
    "[(x,y) for x,y in list(zip(tokenizer.convert_ids_to_tokens(ids), [labels[i] for i in range(len(labels))])) if y in {label2id[\"I-STREET_ADDRESS\"], label2id[\"B-STREET_ADDRESS\"]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bfcd0adbee41639d9cd4e3bb10b354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toke_d1 = AutoTokenizer.from_pretrained('microsoft/deberta-large')\n",
    "\n",
    "tdsd1 = ds1.map(\n",
    "    strided_tokenize,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"tokenizer\": toke_d1, \"stride\": 128, \"max_length\": 512, \"label2id\": label2id}, \n",
    "    batch_size=1,\n",
    "    remove_columns=ds1.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ġ7', 5),\n",
       " ('43', 5),\n",
       " ('ĠE', 0),\n",
       " ('rika', 0),\n",
       " ('ĠBy', 0),\n",
       " ('pass', 0),\n",
       " ('ĠA', 0),\n",
       " ('pt', 0),\n",
       " ('.', 0),\n",
       " ('Ġ419', 0),\n",
       " ('Ċ', 0),\n",
       " ('Andre', 0),\n",
       " ('ah', 0),\n",
       " ('aven', 0),\n",
       " (',', 0),\n",
       " ('ĠIL', 0),\n",
       " ('Ġ54', 0),\n",
       " ('207', 0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2\n",
    "\n",
    "ids = tdsd1[idx][\"input_ids\"]\n",
    "labels = tdsd1[idx][\"labels\"]\n",
    "\n",
    "[(x,y) for x,y in list(zip(toke_d1.convert_ids_to_tokens(ids), [labels[i] for i in range(len(labels))])) if y in {label2id[\"I-STREET_ADDRESS\"], label2id[\"B-STREET_ADDRESS\"]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da7f8c2d16e4594ae7a558868b3d380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toke_d2 = AutoTokenizer.from_pretrained('microsoft/deberta-v2-xlarge')\n",
    "\n",
    "tdsd2 = ds1.map(\n",
    "    strided_tokenize,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"tokenizer\": toke_d2, \"stride\": 128, \"max_length\": 512, \"label2id\": label2id}, \n",
    "    batch_size=1,\n",
    "    remove_columns=ds1.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁743', 5),\n",
       " ('▁Erika', 0),\n",
       " ('▁Bypass', 0),\n",
       " ('▁Apt', 0),\n",
       " ('.', 0),\n",
       " ('▁419', 0),\n",
       " ('▁Andrea', 0),\n",
       " ('haven', 0),\n",
       " (',', 0),\n",
       " ('▁IL', 0),\n",
       " ('▁542', 0),\n",
       " ('07)', 0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 2\n",
    "\n",
    "ids = tdsd2[idx][\"input_ids\"]\n",
    "labels = tdsd2[idx][\"labels\"]\n",
    "\n",
    "[(x,y) for x,y in list(zip(toke_d2.convert_ids_to_tokens(ids), [labels[i] for i in range(len(labels))])) if y in {label2id[\"I-STREET_ADDRESS\"], label2id[\"B-STREET_ADDRESS\"]}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
