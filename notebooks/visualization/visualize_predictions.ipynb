{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this is to make it easier to see how the model makes predictions. This is also to double-check that optimal post-processing is done and no bugs are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accd5a1d2fcb4c17af247d7cf23ee62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/drive2/kaggle/pii-dd/piidd/inference/outputs/d3l-012/char_preds.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m eval_ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_parquet(\u001b[38;5;28mstr\u001b[39m(d \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mds.pq\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     12\u001b[0m tokenized_ds \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_parquet(\u001b[38;5;28mstr\u001b[39m(d \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtds.pq\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 13\u001b[0m char_preds \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchar_preds.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m eidx2tidxs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokenized_ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/drive2/kaggle/pii-dd/piidd/inference/outputs/d3l-012/char_preds.pkl'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "d = Path(\"/drive2/kaggle/pii-dd/piidd/inference/outputs/d3l-012\")\n",
    "model_path = \"/drive2/kaggle/pii-dd/piidd/training/basic/outputs/d3l_012_soup\"\n",
    "\n",
    "np_preds = np.load(str(d / \"preds.npy\"))\n",
    "eval_ds = Dataset.from_parquet(str(d / \"ds.pq\"))\n",
    "tokenized_ds = Dataset.from_parquet(str(d / \"tds.pq\"))\n",
    "char_preds = pickle.load(open(d / \"char_preds.pkl\", \"rb\"))\n",
    "\n",
    "eidx2tidxs = {}\n",
    "\n",
    "for i, x in enumerate(tokenized_ds[\"idx\"]):\n",
    "    if x not in eidx2tidxs:\n",
    "        eidx2tidxs[x] = []\n",
    "    \n",
    "    eidx2tidxs[x].append(i)\n",
    "\n",
    "didx2eidx = {d: i for i, d in enumerate(eval_ds[\"document\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "id2label = json.load(open(str(Path(model_path) / \"config.json\")))[\"id2label\"]\n",
    "\n",
    "def make_table_header(id2label):\n",
    "    html = \"\"\n",
    "    for i in range(len(id2label)):\n",
    "        html += f\"<th>{id2label[str(i)]}</th>\"\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "def generate_html_table(tokens, scores):    \n",
    "    # Start of the HTML string\n",
    "    html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <style>\n",
    "        .highest {\n",
    "            background-color: #66ff33;\n",
    "        }\n",
    "        .medium {\n",
    "            background-color: #ccffff;\n",
    "        }\n",
    "        .low {\n",
    "            background-color: #ff99ff;\n",
    "        }\n",
    "        .tiny {\n",
    "            background-color: #ffb3b3;\n",
    "        }\n",
    "        .zero {\n",
    "            background-color: rgba(0,0,0,0.1);\n",
    "        }\n",
    "        table {\n",
    "            border-collapse: collapse;\n",
    "            width: 100%;\n",
    "        }\n",
    "        th, td {\n",
    "            text-align: center;\n",
    "            padding: 0px;\n",
    "            border: 1px solid black;\n",
    "        }\n",
    "        th {\n",
    "            font-size: 12px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <th>Token</th>\n",
    "    \"\"\" + make_table_header(id2label) + \"\"\"\n",
    "    </tr>\"\"\"\n",
    "    \n",
    "    for token, score in zip(tokens, scores):\n",
    "        row = f\"\\n    <tr>\\n        <td>{token}</td>\"\n",
    "        for s in score:\n",
    "            # Apply high-score class based on the score value\n",
    "            class_name = \"zero\"\n",
    "            if s > 0.9:\n",
    "                class_name = \"highest\"\n",
    "            elif s > 0.7:\n",
    "                class_name = \"medium\"\n",
    "            elif s > 0.5:\n",
    "                class_name = \"low\"\n",
    "            elif s > 0.3:\n",
    "                class_name = \"tiny\"\n",
    "            row += f\"\\n        <td class='{class_name}'>{s:.2f}</td>\"\n",
    "        row += \"\\n    </tr>\"\n",
    "        html += row\n",
    "    \n",
    "    # End of the HTML string\n",
    "    html += \"\"\"\n",
    "</table>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/gradio/route_utils.py\", line 231, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/gradio/blocks.py\", line 1591, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/gradio/blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_466062/606152895.py\", line 28, in load_token_preds\n",
      "    idx = didx2eidx[doc_id]\n",
      "KeyError: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/gradio/route_utils.py\", line 231, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/gradio/blocks.py\", line 1591, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/gradio/blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/nicholas/miniconda3/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_466062/606152895.py\", line 35, in load_token_preds\n",
      "    return show_scores(preds[chunk], offset_mapping[chunk], text)\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "def load_preds(doc_id, do_softmax, char=False):\n",
    "    idx = didx2eidx[doc_id]\n",
    "\n",
    "    if char:\n",
    "        preds = char_preds[idx]\n",
    "    else:\n",
    "        preds = [np_preds[x] for x in eidx2tidxs[idx]]\n",
    "\n",
    "    if do_softmax:\n",
    "        preds = [softmax(x, -1) for x in preds]\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_random_id():\n",
    "\n",
    "    doc_id = random.choice(list(didx2eidx.keys()))\n",
    "\n",
    "    return doc_id\n",
    "\n",
    "\n",
    "def load_token_preds(doc_id, chunk, do_softmax):\n",
    "    idx = didx2eidx[doc_id]\n",
    "\n",
    "    preds = load_preds(doc_id, do_softmax)\n",
    "\n",
    "    offset_mapping = [tokenized_ds[x][\"offset_mapping\"] for x in eidx2tidxs[idx]]\n",
    "    text = eval_ds[idx][\"full_text\"]\n",
    "\n",
    "    return show_scores(preds[chunk], offset_mapping[chunk], text)\n",
    "\n",
    "\n",
    "def load_char_preds(doc_id, chunk, do_softmax):\n",
    "    idx = didx2eidx[doc_id]\n",
    "\n",
    "    preds = load_preds(doc_id, do_softmax, char=True)\n",
    "\n",
    "    text = eval_ds[idx][\"full_text\"]\n",
    "\n",
    "    return show_scores(preds, text, text, chars=True)\n",
    "\n",
    "\n",
    "\n",
    "def show_scores(preds, offset_mapping, text, chars=False):\n",
    "    if chars:\n",
    "        tokens = list(text)\n",
    "    else:\n",
    "        tokens = [text[m[0]:m[1]] for m in offset_mapping]\n",
    "\n",
    "    try:\n",
    "        html = generate_html_table(tokens, preds)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "with gr.Blocks(css=\"\"\".gradio-container {margin: 0 !important; max-width: 4000px};\"\"\") as demo:\n",
    "\n",
    "    rnd = gr.Button(\"Random\")\n",
    "    do_softmax = gr.Checkbox(False, label=\"Softmax\")\n",
    "    doc_id = gr.Slider(0, 1000000, 0, label=\"Doc ID\")\n",
    "    chunk = gr.Slider(0, 10, step=1)\n",
    "\n",
    "\n",
    "    with gr.Tab(\"Model token preds\"):\n",
    "        refresh_token = gr.Button(\"Refresh\")\n",
    "        html_token = gr.HTML()\n",
    "    with gr.Tab(\"Char preds\"):\n",
    "        refresh_char = gr.Button(\"Refresh\")\n",
    "        html_char = gr.HTML()\n",
    "\n",
    "\n",
    "    rnd.click(fn=get_random_id, outputs=doc_id)\n",
    "\n",
    "    refresh_token.click(fn=load_token_preds, inputs=[doc_id, chunk, do_softmax], outputs=html_token)\n",
    "    refresh_char.click(fn=load_char_preds, inputs=[doc_id, chunk, do_softmax], outputs=html_char)\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
