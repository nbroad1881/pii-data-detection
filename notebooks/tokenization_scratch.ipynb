{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d9ebddd051456b8d84f25933b22fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56913d1447b4ac2a5c3d15851e3aa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a2fb5f69754b1994c13eccd986e418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicholas/miniconda3/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"/drive2/kaggle/pii-dd/data/train.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Design', 'O'), ('Thinking', 'O'), ('for', 'O'), ('innovation', 'O'), ('reflexion', 'O'), ('-', 'O'), ('Avril', 'O'), ('2021', 'O'), ('-', 'O'), ('Nathalie', 'B-NAME_STUDENT'), ('Sylla', 'I-NAME_STUDENT'), ('\\n\\n', 'O'), ('Challenge', 'O'), ('&', 'O'), ('selection', 'O'), ('\\n\\n', 'O'), ('The', 'O'), ('tool', 'O'), ('I', 'O'), ('use', 'O')]\n"
     ]
    }
   ],
   "source": [
    "d = data[0]\n",
    "\n",
    "tokens = d[\"tokens\"]\n",
    "labels = d[\"labels\"]\n",
    "tw = d[\"trailing_whitespace\"]\n",
    "\n",
    "print(list(zip(tokens, labels))[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "\n",
    "for t, l, w in zip(tokens, labels, tw):\n",
    "    text += t\n",
    "    if w:\n",
    "        text += \" \"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer(text, return_offsets_mapping=True, return_overflowing_tokens=True, stride=128, max_length=384)\n",
    "len(tokenized.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokenize(example, tokenizer, label2id, max_length, stride):\n",
    "    text = []\n",
    "    char_labels = []\n",
    "\n",
    "    \n",
    "\n",
    "    tokens = example[\"tokens\"][0]\n",
    "    provided_labels = example[\"provided_labels\"][0]\n",
    "    trailing_whitespace = example[\"trailing_whitespace\"][0]\n",
    "\n",
    "    for t, l, ws in zip(\n",
    "        tokens, provided_labels, trailing_whitespace\n",
    "    ):\n",
    "        text.append(t)\n",
    "        char_labels.extend([l] * len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            char_labels.append(\"O\")\n",
    "\n",
    "\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \"\".join(text),\n",
    "        return_offsets_mapping=True,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # tokenized is now a list of lists depending on how long the input is, the max length, and the stride\n",
    "\n",
    "    char_labels = np.array(char_labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = np.ones((len(tokenized.input_ids), max_length), dtype=np.int32) * label2id[\"O\"]\n",
    "\n",
    "    for i in range(len(tokenized.input_ids)):\n",
    "\n",
    "        for j, (start_idx, end_idx) in enumerate(tokenized.offset_mapping[i]):\n",
    "            # CLS token\n",
    "            if start_idx == 0 and end_idx == 0:\n",
    "                continue\n",
    "\n",
    "            # case when token starts with whitespace\n",
    "            while text[start_idx].isspace():\n",
    "                start_idx += 1  \n",
    "\n",
    "            \n",
    "            start_idx = min(start_idx, len(char_labels) - 1)\n",
    "\n",
    "            token_labels[i, j] = label2id[char_labels[start_idx]]\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_labels = {\n",
    "    \"EMAIL\",\n",
    "    \"ID_NUM\",\n",
    "    \"NAME_STUDENT\",\n",
    "    \"PHONE_NUM\",\n",
    "    \"STREET_ADDRESS\",\n",
    "    \"URL_PERSONAL\",\n",
    "    \"USERNAME\",\n",
    "}\n",
    "all_labels = []\n",
    "for l in base_labels:\n",
    "    all_labels.append(f\"B-{l}\")\n",
    "    all_labels.append(f\"I-{l}\")\n",
    "all_labels.append(\"O\")\n",
    "\n",
    "all_labels = sorted(all_labels)\n",
    "label2id = {l: i for i, l in enumerate(all_labels)}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "ds1 = Dataset.from_dict(\n",
    "        {\n",
    "            \"full_text\": [x[\"full_text\"] for x in data],\n",
    "            # \"document\": [x[\"document\"] for x in data],\n",
    "            \"tokens\": [x[\"tokens\"] for x in data],\n",
    "            \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "            \"provided_labels\": [x[\"labels\"] for x in data],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193d0bfce2c54ba79edce93af1ea2d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tds = ds1.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": 384, \"stride\": 128}, remove_columns=ds1.column_names, batched=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 8837,\n",
       " 2969,\n",
       " 1489,\n",
       " 262,\n",
       " 2362,\n",
       " 261,\n",
       " 306,\n",
       " 403,\n",
       " 2537,\n",
       " 579,\n",
       " 262,\n",
       " 780,\n",
       " 1017,\n",
       " 261,\n",
       " 2128,\n",
       " 2177,\n",
       " 360,\n",
       " 602,\n",
       " 864,\n",
       " 265,\n",
       " 9608,\n",
       " 267,\n",
       " 5646,\n",
       " 263,\n",
       " 1236,\n",
       " 3478,\n",
       " 2588,\n",
       " 261,\n",
       " 269,\n",
       " 449,\n",
       " 4515,\n",
       " 262,\n",
       " 1933,\n",
       " 296,\n",
       " 298,\n",
       " 1067,\n",
       " 264,\n",
       " 1348,\n",
       " 271,\n",
       " 1492,\n",
       " 715,\n",
       " 38217,\n",
       " 9608,\n",
       " 1262,\n",
       " 293,\n",
       " 262,\n",
       " 513,\n",
       " 265,\n",
       " 16789,\n",
       " 260,\n",
       " 7616,\n",
       " 261,\n",
       " 401,\n",
       " 265,\n",
       " 262,\n",
       " 1348,\n",
       " 271,\n",
       " 1492,\n",
       " 715,\n",
       " 38217,\n",
       " 1074,\n",
       " 22858,\n",
       " 263,\n",
       " 262,\n",
       " 671,\n",
       " 276,\n",
       " 268,\n",
       " 22423,\n",
       " 5646,\n",
       " 1706,\n",
       " 261,\n",
       " 2076,\n",
       " 770,\n",
       " 278,\n",
       " 1037,\n",
       " 1446,\n",
       " 277,\n",
       " 462,\n",
       " 3478,\n",
       " 263,\n",
       " 14340,\n",
       " 260,\n",
       " 329,\n",
       " 9482,\n",
       " 269,\n",
       " 461,\n",
       " 38459,\n",
       " 261,\n",
       " 304,\n",
       " 262,\n",
       " 671,\n",
       " 296,\n",
       " 286,\n",
       " 264,\n",
       " 3566,\n",
       " 262,\n",
       " 2943,\n",
       " 10980,\n",
       " 267,\n",
       " 469,\n",
       " 384,\n",
       " 628,\n",
       " 261,\n",
       " 263,\n",
       " 1446,\n",
       " 283,\n",
       " 28890,\n",
       " 283,\n",
       " 628,\n",
       " 260,\n",
       " 325,\n",
       " 327,\n",
       " 303,\n",
       " 264,\n",
       " 413,\n",
       " 469,\n",
       " 1016,\n",
       " 272,\n",
       " 295,\n",
       " 825,\n",
       " 262,\n",
       " 1933,\n",
       " 939,\n",
       " 497,\n",
       " 1114,\n",
       " 2177,\n",
       " 260,\n",
       " 545,\n",
       " 291,\n",
       " 1956,\n",
       " 266,\n",
       " 310,\n",
       " 9939,\n",
       " 263,\n",
       " 1636,\n",
       " 671,\n",
       " 260,\n",
       " 3289,\n",
       " 261,\n",
       " 385,\n",
       " 299,\n",
       " 2262,\n",
       " 8909,\n",
       " 265,\n",
       " 1506,\n",
       " 261,\n",
       " 278,\n",
       " 1015,\n",
       " 264,\n",
       " 286,\n",
       " 32601,\n",
       " 352,\n",
       " 266,\n",
       " 6152,\n",
       " 260,\n",
       " 589,\n",
       " 262,\n",
       " 2943,\n",
       " 831,\n",
       " 261,\n",
       " 1280,\n",
       " 387,\n",
       " 11008,\n",
       " 310,\n",
       " 497,\n",
       " 49649,\n",
       " 262,\n",
       " 4472,\n",
       " 2381,\n",
       " 337,\n",
       " 278,\n",
       " 2484,\n",
       " 264,\n",
       " 1067,\n",
       " 264,\n",
       " 5646,\n",
       " 20166,\n",
       " 360,\n",
       " 262,\n",
       " 2434,\n",
       " 1384,\n",
       " 377,\n",
       " 270,\n",
       " 738,\n",
       " 261,\n",
       " 293,\n",
       " 1680,\n",
       " 723,\n",
       " 2437,\n",
       " 3666,\n",
       " 4786,\n",
       " 390,\n",
       " 3618,\n",
       " 261,\n",
       " 263,\n",
       " 12998,\n",
       " 264,\n",
       " 3071,\n",
       " 263,\n",
       " 6961,\n",
       " 5646,\n",
       " 1687,\n",
       " 275,\n",
       " 266,\n",
       " 23442,\n",
       " 376,\n",
       " 927,\n",
       " 14882,\n",
       " 10745,\n",
       " 28662,\n",
       " 2142,\n",
       " 5646,\n",
       " 3970,\n",
       " 260,\n",
       " 344,\n",
       " 908,\n",
       " 264,\n",
       " 14488,\n",
       " 261,\n",
       " 278,\n",
       " 403,\n",
       " 2716,\n",
       " 613,\n",
       " 2190,\n",
       " 1695,\n",
       " 2112,\n",
       " 270,\n",
       " 277,\n",
       " 271,\n",
       " 5266,\n",
       " 1296,\n",
       " 261,\n",
       " 264,\n",
       " 413,\n",
       " 1951,\n",
       " 265,\n",
       " 469,\n",
       " 926,\n",
       " 265,\n",
       " 594,\n",
       " 58094,\n",
       " 260,\n",
       " 279,\n",
       " 763,\n",
       " 926,\n",
       " 265,\n",
       " 58094,\n",
       " 637,\n",
       " 1127,\n",
       " 334,\n",
       " 266,\n",
       " 2854,\n",
       " 1062,\n",
       " 260,\n",
       " 1304,\n",
       " 671,\n",
       " 263,\n",
       " 613,\n",
       " 2190,\n",
       " 6967,\n",
       " 286,\n",
       " 13133,\n",
       " 1311,\n",
       " 267,\n",
       " 4228,\n",
       " 3599,\n",
       " 893,\n",
       " 261,\n",
       " 263,\n",
       " 421,\n",
       " 461,\n",
       " 403,\n",
       " 282,\n",
       " 77806,\n",
       " 270,\n",
       " 1296,\n",
       " 260,\n",
       " 1289,\n",
       " 337,\n",
       " 1160,\n",
       " 333,\n",
       " 298,\n",
       " 413,\n",
       " 470,\n",
       " 1587,\n",
       " 261,\n",
       " 13420,\n",
       " 270,\n",
       " 1296,\n",
       " 261,\n",
       " 283,\n",
       " 371,\n",
       " 283,\n",
       " 299,\n",
       " 1577,\n",
       " 326,\n",
       " 1108,\n",
       " 261,\n",
       " 296,\n",
       " 527,\n",
       " 4472,\n",
       " 2381,\n",
       " 1761,\n",
       " 8973,\n",
       " 262,\n",
       " 671,\n",
       " 269,\n",
       " 1721,\n",
       " 314,\n",
       " 14158,\n",
       " 5646,\n",
       " 4819,\n",
       " 260,\n",
       " 18535,\n",
       " 264,\n",
       " 671,\n",
       " 2177,\n",
       " 261,\n",
       " 262,\n",
       " 896,\n",
       " 296,\n",
       " 282,\n",
       " 264,\n",
       " 18307,\n",
       " 260,\n",
       " 11149,\n",
       " 53878,\n",
       " 558,\n",
       " 269,\n",
       " 266,\n",
       " 1367,\n",
       " 263,\n",
       " 3457,\n",
       " 666,\n",
       " 265,\n",
       " 1306,\n",
       " 4127,\n",
       " 3478,\n",
       " 263,\n",
       " 403,\n",
       " 282,\n",
       " 66445,\n",
       " 283,\n",
       " 858,\n",
       " 260,\n",
       " 5474,\n",
       " 262,\n",
       " 2231,\n",
       " 265,\n",
       " 262,\n",
       " 36011,\n",
       " 261,\n",
       " 310,\n",
       " 1670,\n",
       " 1677,\n",
       " 9299,\n",
       " 264,\n",
       " 262,\n",
       " 23193,\n",
       " 8008,\n",
       " 261,\n",
       " 869,\n",
       " 267,\n",
       " 3599,\n",
       " 893,\n",
       " 272,\n",
       " 333,\n",
       " 298,\n",
       " 286,\n",
       " 739,\n",
       " 264,\n",
       " 11149,\n",
       " 53878,\n",
       " 558,\n",
       " 261,\n",
       " 269,\n",
       " 23656,\n",
       " 260,\n",
       " 279,\n",
       " 671,\n",
       " 263,\n",
       " 613,\n",
       " 2]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "[CLS] REFLECTION – LEARNING LAUNCH Irfan Khan Reflection – Learning Launch Challenge I was recently promoted as the team leader of a small team consisting of two (2) web developers, one (1) graphic designer, one (1) marketing associate and one (1) lead generation specialist and my/our challenge is to create an effective workflow within the team in order to successfully develop and launch our client’s brand. The project scope includes building a brand, starting a marketing campaign and launch a website. The team of five (5) employees will work on this project with an even distribution of working hours amongst the team members. The completion of this project should not exceed 45 business days. We will focus on trying to determine what is the best workflow that works for our team. Selection This will be the first design thinking activities/tools that our team will try, and we chose the learning launch tool because we think that this is the best way for us to determine what workflow structure will best work for our team. Application For our first learning launch, we focused on the structure of our task delegation and management. We decided to use Trello for tasks management and tracking the progress of each task and Slack as our tool of communication. We created one Trello card for each task and assigned them to the specific team members. We also created two (2) additional Trello cards, namely, In Progress and Completed and as the name suggest, these cards are used to track the progress of each of the tasks. This tested our assumption that using a structure for task delegation and management were the main causes of our workflow challenge. We have an assumption that if we will not use a project management software for task delegation and management, we will have a hard time tracking the progress of each tasks and worst we will not complete the project on time. We expected that if we continue to operate using this workflow and it will benefit our team in our future projects[SEP]\n",
      "('▁Irfan', 'B-NAME_STUDENT')\n",
      "('▁Khan', 'I-NAME_STUDENT')\n"
     ]
    }
   ],
   "source": [
    "# Confirm that alignment is good\n",
    "\n",
    "# run multiple times to see different rows\n",
    "x = tds.shuffle()[0]\n",
    "\n",
    "\n",
    "print(\"*\"*100)\n",
    "\n",
    "print(tokenizer.decode(x[\"input_ids\"]))\n",
    "       \n",
    "for t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n",
    "    if id2label[l] != \"O\":\n",
    "        print((t,id2label[l]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '’', 's', '▁income', '▁for', '▁livelihood', '.', '▁Because', '▁of', '▁the', '▁sudden', '▁lockdown', ',', '▁Santosh', '▁was', '▁unaware', '▁of', '▁the', '▁scenario', '▁and', '▁he', '▁and', '▁his', '▁family', '▁are', '▁in', '▁Gujrat', '▁and', '▁his', '▁parents', '▁are', '▁in', '▁Odisha', '.', '▁The', '▁production', '▁in', '▁his', '▁factory', '▁is', '▁at', '▁halt', ',', '▁and', '▁since', '▁he', '▁is', '▁a', '▁daily', '▁worker', ',', '▁he', '▁is', '▁left', '▁with', '▁no', '▁source', '▁of', '▁income', '▁and', '▁fails', '▁to', '▁feed', '▁his', '▁family', '.', '▁He', '▁has', '▁requested', '▁the', '▁government', '▁to', '▁allow', '▁him', '▁move', '▁to', '▁his', '▁home', '▁state', '▁where', '▁he', '▁can', '▁at', '▁least', '▁work', '▁as', '▁a', '▁farmer', '.', '▁His', '▁request', '▁has', '▁been', '▁denied', '▁by', '▁the', '▁government', '▁and', '▁now', '▁he', '▁has', '▁no', '▁idea', '▁what', '▁he', '▁should', '▁do', '▁to', '▁feed', '▁his', '▁wife', ',', '▁two', '▁daughters', '▁and', '▁old', '▁parents', '.', '▁Insights', '▁and', '▁App', 'roch', '▁This', '▁is', '▁one', '▁of', '▁the', '▁ugliest', '▁realities', '▁in', '▁India', '.', '▁We', '▁have', '▁no', '▁provisions', '▁for', '▁the', '▁unskilled', '▁workers', '.', '▁Even', '▁though', '▁certain', '▁government', '▁schemes', '▁were', '▁launched', ',', '▁it', '▁could', '▁not', '▁deliver', '▁as', '▁expected', '.', '▁Workers', '▁like', '▁Santosh', ',', '▁they', '▁didn', '’', 't', '▁get', '▁a', '▁proper', '▁education', '▁and', '▁because', '▁of', '▁family', '▁responsibilities', '▁they', '▁enter', '▁into', '▁the', '▁labor', '▁market', '▁at', '▁an', '▁early', '▁age', ',', '▁as', '▁a', '▁result', '▁they', '▁can', '▁not', '▁up', 'skill', '▁themselves', '▁with', '▁passage', '▁of', '▁time', '▁and', '▁get', '▁stuck', '▁doing', '▁the', '▁same', '▁work', '▁till', '▁there', '▁physical', '▁health', '▁permits', '▁and', '▁are', '▁left', '▁underpaid', '▁throughout', '▁their', '▁life', '.', '▁They', '▁have', '▁no', '▁scope', '▁for', '▁financial', '▁growth', '▁and', '▁are', '▁left', '▁with', '▁high', '▁financial', '▁insecurities', '▁and', '▁responsibilities', '.', '▁Though', '▁the', '▁state', '▁governments', '▁are', '▁coming', '▁up', '▁with', '▁effective', '▁schemes', '▁and', '▁programs', '▁for', '▁them', ',', '▁but', '▁these', '▁migrant', '▁workers', '▁remain', '▁unaware', '▁of', '▁the', '▁schemes', '.', '▁They', '▁are', '▁the', '▁deprived', '▁people', '▁in', '▁the', '▁society', '▁without', '▁any', '▁social', '▁safety', '▁or', '▁healthcare', '▁benefit', '.', '▁It', '▁has', '▁been', '▁noticed', '▁that', '▁most', '▁of', '▁them', '▁don', '’', 't', '▁even', '▁have', '▁voter', '▁ID', '▁cards', '.', '▁Everyday', '▁we', '▁see', '▁in', '▁news', '▁how', '▁desperately', '▁these', '▁workers', '▁try', '▁to', '▁escape', '▁and', '▁run', '▁away', '▁to', '▁their', '▁respective', '▁homes', '▁to', '▁their', '▁families', '▁amid', '▁this', '▁crisis', '.', '▁It', '▁is', '▁really', '▁sad', '▁to', '▁see', '▁how', '▁desperate', '▁these', '▁people', '▁are', '.', '▁Every', '▁day', '▁we', '▁see', '▁videos', '▁in', '▁news', '▁channels', '▁where', '▁women', '▁and', '▁children', '▁are', '▁walking', '▁miles', '▁to', '▁reach', '▁their', '▁home', '▁or', '▁to', '▁their', '▁families', '.', '▁People', '▁every', '▁day', '▁are', '▁dying', '▁because', '▁of', '▁starvation', '.', '▁In', '▁spite', '▁of', '▁creating', '▁so', '▁much', '▁awareness', '▁about', '▁the', '▁issue', '▁some', '▁of', '▁them', '▁move', '▁along', '▁the', '▁roads', '▁to', '▁get', '▁back', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71691552, 0.64349265, 0.85438092, 0.58456591, 0.10275883,\n",
       "        0.36860505, 0.51464224, 0.46502847],\n",
       "       [0.41150537, 0.38086034, 0.36238575, 0.95276757, 0.92933539,\n",
       "        0.36860505, 0.60820741, 0.47948528],\n",
       "       [0.75110017, 0.80993238, 0.52904256, 0.58456591, 0.90332888,\n",
       "        0.48367086, 0.62744279, 0.76127702],\n",
       "       [0.76630244, 0.42193554, 0.36238575, 0.58456591, 0.70000835,\n",
       "        0.36860505, 0.68136607, 0.25338795]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = np.random.rand(4, 8)\n",
    "p = np.random.rand(1, 8)\n",
    "\n",
    "np.maximum(n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71691552, 0.64349265, 0.85438092, 0.04943591, 0.10275883,\n",
       "        0.02191087, 0.51464224, 0.46502847],\n",
       "       [0.41150537, 0.38086034, 0.34002642, 0.95276757, 0.92933539,\n",
       "        0.26779906, 0.60820741, 0.47948528],\n",
       "       [0.75110017, 0.80993238, 0.52904256, 0.3263116 , 0.90332888,\n",
       "        0.48367086, 0.62744279, 0.76127702],\n",
       "       [0.76630244, 0.42193554, 0.35928957, 0.25104063, 0.70000835,\n",
       "        0.10567496, 0.68136607, 0.04068233]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01843215, 0.33613288, 0.36238575, 0.58456591, 0.07117521,\n",
       "        0.36860505, 0.12551771, 0.25338795]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
