model_path: microsoft/deberta-v3-large
max_length: 512
stride: 128
model_dtype: fp32
use_lora: no
use_dora: yes
lora_r: 32
lora_dropout: 0.1
lora_modules: "all-linear"
layer_drop_prob: 0.1
use_multisample_dropout: no

train_on_all_data: yes
filter_no_pii_percent_allow: 0.3
add_newline_token: yes
random_seed: yes
fold: 2
remove_classes: "I-URL_PERSONAL,I-EMAIL,I-USERNAME"
remove_bi: no

main_dataset_path: "/drive2/kaggle/pii-dd/data/corrected_train_v2.json"
extra_dataset_path: "/drive2/kaggle/pii-dd/data/mixtral-v3.json"
num_extra_samples: 3000

num_proc: 8
debugg: no
wandb_run_group: d3l-med

# training_args:
output_dir: "outputs/d3l-mixv3"
num_train_epochs: 2
learning_rate: 3e-5
lr_scheduler_type: "cosine"
weight_decay: 0.01
warmup_ratio: 0.1
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 1
gradient_checkpointing: no
fp16: yes
logging_steps: 50
evaluation_strategy: "epoch"
save_strategy: "epoch"
save_total_limit: 1
dataloader_num_workers: 1
metric_for_best_model: "f5_score"
greater_is_better: yes
report_to: "wandb"
optim: "adamw_8bit"
max_grad_norm: 1.0
seed: 42


